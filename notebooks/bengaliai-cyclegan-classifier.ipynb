{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./efficientnetpytorch\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from efficientnet-pytorch==0.7.0) (1.4.0)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-py3-none-any.whl size=20139 sha256=6bed8db746400366305c137f17190314c41637d96ae5d61ed5cf31d47ee89fbb\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ivtibaq3/wheels/7a/d7/95/28482cfaa1c31d6109bf678dd4d51a251ec23922667909d924\r\n",
      "Successfully built efficientnet-pytorch\r\n",
      "Installing collected packages: efficientnet-pytorch\r\n",
      "Successfully installed efficientnet-pytorch-0.7.0\r\n"
     ]
    }
   ],
   "source": [
    "!cp -r ../input/efficientnetpytorch/ ./efficientnetpytorch\n",
    "!pip install ./efficientnetpytorch/\n",
    "!rm -r ./efficientnetpytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import gc\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [0.5, 0.5, 0.5]\n",
    "STD = [0.5, 0.5, 0.5]\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 40\n",
    "TQDM_DISABLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(paths):\n",
    "    all_images = []\n",
    "    for path in paths:\n",
    "        image_df = pd.read_parquet(path)\n",
    "        images = image_df.iloc[:, 1:].values.reshape(-1, 137, 236).astype(np.uint8)\n",
    "        del image_df\n",
    "        gc.collect()\n",
    "        all_images.append(images)\n",
    "    all_images = np.concatenate(all_images)\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('../input/bengaliai-cv19/train.csv')\n",
    "# multi_diacritics_train_data = pd.read_csv('../input/bengaliai-cv19/train_multi_diacritics.csv')\n",
    "# train_data = train_data.set_index('image_id')\n",
    "# multi_diacritics_train_data = multi_diacritics_train_data.set_index('image_id')\n",
    "# train_data.update(multi_diacritics_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images = load_images([\n",
    "#     '../input/bengaliai-cv19/train_image_data_0.parquet',\n",
    "#     '../input/bengaliai-cv19/train_image_data_1.parquet',\n",
    "#     '../input/bengaliai-cv19/train_image_data_2.parquet',\n",
    "#     '../input/bengaliai-cv19/train_image_data_3.parquet',\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_data = pd.read_csv('../input/bengaliai-cv19-font/font.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_images = load_images([\n",
    "    '../input/bengaliai-cv19-font/font_image_data_0.parquet',\n",
    "    '../input/bengaliai-cv19-font/font_image_data_1.parquet',\n",
    "    '../input/bengaliai-cv19-font/font_image_data_2.parquet',\n",
    "    '../input/bengaliai-cv19-font/font_image_data_3.parquet',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create  Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphemeDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, images, transform=None, num_grapheme_root=168, num_vowel_diacritic=11, num_consonant_diacritic=8):\n",
    "        self.data = data\n",
    "        self.grapheme_root_list = np.array(data['grapheme_root'].tolist(), dtype=np.int64)\n",
    "        self.vowel_diacritic_list = np.array(data['vowel_diacritic'].tolist(), dtype=np.int64)\n",
    "        self.consonant_diacritic_list = np.array(data['consonant_diacritic'].tolist(), dtype=np.int64)\n",
    "        self.num_grapheme_root = num_grapheme_root\n",
    "        self.num_vowel_diacritic = num_vowel_diacritic\n",
    "        self.num_consonant_diacritic = num_consonant_diacritic\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        grapheme_root = self.grapheme_root_list[idx]\n",
    "        vowel_diacritic = self.vowel_diacritic_list[idx]\n",
    "        consonant_diacritic = self.consonant_diacritic_list[idx]\n",
    "        label = (grapheme_root*self.num_vowel_diacritic+vowel_diacritic)*self.num_consonant_diacritic+consonant_diacritic\n",
    "        np_image = self.images[idx].copy()\n",
    "        out_image = self.transform(np_image)\n",
    "        return out_image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Albumentations:\n",
    "    def __init__(self, augmentations):\n",
    "        self.augmentations = A.Compose(augmentations)\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        image = self.augmentations(image=image)['image']\n",
    "        return image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = [\n",
    "    A.CenterCrop(height=137, width=IMG_WIDTH),\n",
    "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, always_apply=True),\n",
    "]\n",
    "\n",
    "augmentations = [\n",
    "    A.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], always_apply=True),\n",
    "    A.imgaug.transforms.IAAAffine(shear=20, mode='constant', cval=255, always_apply=True),\n",
    "    A.ShiftScaleRotate(rotate_limit=20, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], mask_value=[255, 255, 255], always_apply=True),\n",
    "    A.RandomCrop(height=IMG_HEIGHT, width=IMG_WIDTH, always_apply=True),\n",
    "    A.Cutout(num_holes=1, max_h_size=112, max_w_size=112, fill_value=128, always_apply=True),\n",
    "]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    np.uint8,\n",
    "    transforms.Lambda(lambda x: np.array([x, x, x]).transpose((1, 2, 0)) ),\n",
    "    np.uint8,\n",
    "    Albumentations(preprocess + augmentations),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "#     transforms.ToPILImage(),\n",
    "])\n",
    "valid_transform = transforms.Compose([\n",
    "    np.uint8,\n",
    "    transforms.Lambda(lambda x: np.array([x, x, x]).transpose((1, 2, 0)) ),\n",
    "    np.uint8,\n",
    "    Albumentations(preprocess),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "#     transforms.ToPILImage(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_dataset = GraphemeDataset(font_data, font_images, train_transform)\n",
    "valid_dataset = GraphemeDataset(font_data, font_images, valid_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengalModel(nn.Module):\n",
    "    def __init__(self, backbone, hidden_size=2560, class_num=168*11*7):\n",
    "        super(BengalModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(hidden_size, class_num)\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        bs = inputs.shape[0]\n",
    "        feature = self.backbone.extract_features(inputs)\n",
    "        feature_vector = self._avg_pooling(feature)\n",
    "        feature_vector = feature_vector.view(bs, -1)\n",
    "        feature_vector = self.ln(feature_vector)\n",
    "\n",
    "        out = self.fc(feature_vector)\n",
    "        return out   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = EfficientNet.from_name('efficientnet-b1')\n",
    "classifier = BengalModel(backbone, hidden_size=1280, class_num=168*11*8).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_sampler = torch.utils.data.RandomSampler(font_dataset, True, int(len(font_dataset))*(EPOCH))\n",
    "valid_sampler = torch.utils.data.RandomSampler(valid_dataset, True, int(len(valid_dataset))*(EPOCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_loader = torch.utils.data.DataLoader(\n",
    "    font_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=1, \n",
    "    pin_memory=True, \n",
    "    drop_last=True, \n",
    "    sampler=font_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_loader_iter = iter(font_loader)\n",
    "valid_loader_iter = iter(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, train_iter, criterion, optimizer, scheduler, device):\n",
    "    image, label = next(train_iter)\n",
    "    image = image.to(device)\n",
    "    label = label.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    out = model(image)\n",
    "    loss = criterion(out, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_step_per_epoch = len(font_loader)//EPOCH\n",
    "num_valid_step_per_epoch = len(valid_loader)//EPOCH\n",
    "train_steps = num_step_per_epoch*EPOCH\n",
    "WARM_UP_STEP = train_steps*0.5\n",
    "\n",
    "def warmup_linear_decay(step):\n",
    "    if step < WARM_UP_STEP:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return (train_steps-step)/(train_steps-WARM_UP_STEP)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_linear_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "best_score = 0.\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    classifier.train()\n",
    "    metric = {}\n",
    "    losses = []\n",
    "    for i in tqdm(range(num_step_per_epoch), disable=TQDM_DISABLE):\n",
    "        loss = train_step(classifier,\n",
    "                  font_loader_iter,\n",
    "                  classifier_loss,\n",
    "                  optimizer,\n",
    "                  scheduler,\n",
    "                  device)        \n",
    "        losses.append(loss.item())\n",
    "    metric['train/loss'] = sum(losses)/len(losses)\n",
    "    classifier.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i in tqdm(range(num_valid_step_per_epoch), disable=TQDM_DISABLE):\n",
    "        image, label = next(valid_loader_iter)\n",
    "        image = image.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = classifier(image)\n",
    "            pred = out.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        preds.append(pred)\n",
    "        labels.append(label.numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_pred=preds, y_true=labels)\n",
    "    metric['valid/accuracy'] = accuracy\n",
    "    metric['epoch'] = epoch\n",
    "    \n",
    "    log.append(metric)\n",
    "    \n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        torch.save(classifier.state_dict(), 'best.pth')\n",
    "    torch.save(classifier.state_dict(), 'model.pth')\n",
    "    with open('log.json', 'w') as fout:\n",
    "        json.dump(log , fout, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
